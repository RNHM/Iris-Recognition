{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gabor_Feature_Extraction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEz7AAsgSHjl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJyb00KTSY_e"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "test_data = np.load(\"/content/drive/My Drive/DataShuffled/training_images\", allow_pickle = True)\n",
        "test_labels = np.load(\"/content/drive/My Drive/DataShuffled/training_subjects\", allow_pickle = True)\n",
        "train_data = np.load(\"/content/drive/My Drive/DataShuffled/test_images\", allow_pickle = True)\n",
        "train_labels = np.load(\"/content/drive/My Drive/DataShuffled/test_subjects\", allow_pickle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqNqI6BwSZi0"
      },
      "source": [
        "# Reduce size of image from 360x102 to 360x100 by extracting two uttermost bottom pixels.\n",
        "\n",
        "test_data_r = np.zeros((1120, 100, 360))\n",
        "train_data_r = np.zeros((1120, 100, 360))\n",
        "\n",
        "for i in range(test_data.shape[0]):\n",
        "  test_data_r[i, :,:] = test_data[i, 0:100, :]\n",
        "\n",
        "for i in range(train_data.shape[0]):\n",
        "  train_data_r[i, :,:] = train_data[i, 0:100, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AghRasJ0SjNV"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pylab as pl\n",
        "import glob\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# define gabor filter bank with different orientations and at different scales\n",
        "def build_filters():\n",
        "  num = 0\n",
        "  theta = 0\n",
        "  lamda = 2\n",
        "  kernels = []\n",
        "  data = np.zeros((24, 4))\n",
        "  for lamda in [1.4, 1.7, 2.0, 2.3]:\n",
        "    for sigma in [1.4, 1.8]:\n",
        "      for gamma in [0, 1]: # Define number of thetas\n",
        "        for i in [0, 2, 4, 6]:\n",
        "          theta = math.pi*i/8\n",
        "          gabor_label = 'Gabor' + str(num)\n",
        "          ksize = 25\n",
        "          kernel = cv2.getGaborKernel((ksize,ksize), sigma, theta, lamda, gamma, 0, ktype = cv2.CV_32F)\n",
        "          kernels.append(kernel)\n",
        "          data[num, 0] = theta\n",
        "          data[num, 1] = sigma\n",
        "          data[num, 2] = lamda\n",
        "          data[num, 3] = gamma\n",
        "          print(gabor_label, ' : theta = ', theta, ' : sigma = ', sigma, ' : lambda = ', lamda, ' : gamma = ', gamma)\n",
        "          num = num + 1\n",
        "  return kernels, data\n",
        "\n",
        "#function to convolve the image with the filters\n",
        "def process(img, filters):\n",
        "  accum = np.zeros_like(img)\n",
        "  fimg = cv2.filter2D(img, cv2.CV_8UC3, filters)\n",
        "  np.maximum(accum, fimg, accum)\n",
        "  return accum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3bE8f4hS0za"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def FeatureExtraction(image, filterBank):\n",
        "\n",
        "  #initializing the feature vector\n",
        "  f_range = len(filterBank)\n",
        "  vector = []\n",
        "  for m in range(f_range):\n",
        "    img_check = process(image, filterBank[m])\n",
        "    i = 0\n",
        "    k = 0\n",
        "    while i < img_check.shape[0]:\n",
        "      j = 0\n",
        "      while j < img_check.shape[1]:\n",
        "        mean_img = img_check[i:i+25, j:j+60].mean()\n",
        "        AAD = abs(img_check[i:i+25, j:j+60] - mean_img).mean()\n",
        "        \n",
        "\n",
        "        vector.append(mean_img)\n",
        "        vector.append(AAD)\n",
        "\n",
        "        j = j + 60\n",
        "      i = i + 25\n",
        "  return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03EP7ppKS9PW"
      },
      "source": [
        "filters = build_filters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os2H78rNS04K"
      },
      "source": [
        "from tqdm import tqdm\n",
        "feature_vector_train = np.zeros((1120, 2304))\n",
        "\n",
        "for i in range(train_data.shape[0]):\n",
        "  feature_vector_train[i, :] = FeatureExtraction(train_data_r[i], filters[0])\n",
        "\n",
        "feature_vector_test = np.zeros((1118, 2304)) \n",
        "\n",
        "for i in (range(test_data.shape[0])):\n",
        " feature_vector_test[i, :] = FeatureExtraction(test_data_r[i], filters[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4G1U9gmTITb"
      },
      "source": [
        "# Shuffle train_data with labels : \n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "ind_list = [i for i in range(1120)]\n",
        "shuffle(ind_list)\n",
        "train_features  = feature_vector_train[ind_list]\n",
        "y_train= train_labels[ind_list]\n",
        "\n",
        "\n",
        "#Apply PCA :\n",
        "from sklearn.decomposition import PCA \n",
        "\n",
        "pca = PCA(.99, whiten= True)\n",
        "pca.fit(train_features)\n",
        "train_features_pca = pca.transform(train_features)\n",
        "test_features_pca = pca.transform(feature_vector_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yp0qgSlTS2n"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As3f-UdNTQl1"
      },
      "source": [
        "#Apply Classifier\n",
        "import sklearn\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score,roc_curve, auc\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(train_features_pca, y_train)\n",
        "\n",
        "scores = knn.predict(train_features_pca)\n",
        "\n",
        "# Show train accuracy\n",
        "print('\\nPrediction accuracy:')\n",
        "print('{:.2%}\\n'.format(accuracy_score(y_train, scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8OyAEIYTXCy"
      },
      "source": [
        "# function to transform labels to one hot :\n",
        "\n",
        "def one_hot(labels):\n",
        "  one_hot = []\n",
        "  for i in range(len(labels)):\n",
        "    y = np.zeros((224))\n",
        "    y[labels[i]]=1\n",
        "    one_hot.append(y)\n",
        "  one_hot = np.array(one_hot)\n",
        "  return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbMIcmP_TVT5"
      },
      "source": [
        "y_pred = knn.predict(test_features_pca)\n",
        "\n",
        "# Show prediction accuracy\n",
        "print('\\nPrediction accuracy:')\n",
        "print('{:.2%}\\n'.format(accuracy_score(test_labels, y_pred)))\n",
        "print('\\nPrecision:')\n",
        "print('{:.2%}\\n'.format(precision_score(test_labels, y_pred,average='weighted')))\n",
        "print('\\nrecall:')\n",
        "print('{:.2%}\\n'.format(recall_score(test_labels, y_pred,average='weighted')))\n",
        "print('\\nf1 score:')\n",
        "print('{:.2%}\\n'.format(f1_score(test_labels, y_pred,average='weighted')))\n",
        "print('\\n auc:')\n",
        "print('{:.2%}\\n'.format(roc_auc_score(one_hot(test_labels), one_hot(y_pred), average='weighted', multi_class='ovo')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwAOsGLHTcBB"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On-XxE_pTbYv"
      },
      "source": [
        "#SVM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()),('classifier',SVC())])\n",
        "\n",
        "params = dict()\n",
        "params['classifier__C'] = np.logspace(-5, 5, 10)\n",
        "params['classifier__kernel'] = ['linear', 'poly', 'rbf']\n",
        "params['classifier__gamma'] = [0.01, 0.1, 1]\n",
        "\n",
        "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "\n",
        "grid_search_svm = GridSearchCV(pipe, params, cv = cv)\n",
        "grid_search_svm.fit(train_features_pca, y_train)\n",
        "\n",
        "print(\"The best parameters are %s with an accuracy of %0.4f\"%(grid_search_svm.best_params_, grid_search_svm.best_score_))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRf1GUotTlUi"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "y_pred= grid_search_svm.predict(test_features_pca)\n",
        "cm = confusion_matrix(y_pred, test_labels)\n",
        "\n",
        "# Svm metrics \n",
        "\n",
        "print('\\nPrediction accuracy:')\n",
        "print('{:.2%}\\n'.format(accuracy_score(test_labels, y_pred)))\n",
        "print('\\nPrecision:')\n",
        "print('{:.2%}\\n'.format(precision_score(test_labels, y_pred,average='weighted')))\n",
        "print('\\nrecall:')\n",
        "print('{:.2%}\\n'.format(recall_score(test_labels, y_pred,average='weighted')))\n",
        "print('\\nf1 score:')\n",
        "print('{:.2%}\\n'.format(f1_score(test_labels, y_pred,average='weighted')))\n",
        "print('\\n auc:')\n",
        "print('{:.2%}\\n'.format(roc_auc_score(one_hot(test_labels), one_hot(y_pred), average='weighted', multi_class='ovo')))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}